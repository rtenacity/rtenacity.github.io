<!DOCTYPE html>
<html lang="en" dir=" auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Linear Regression and Free Will | rohanarni.com</title>
<meta name="keywords" content="">
<meta name="description" content="Do we have free will? That’s a question that’s haunted many philosophers, scientists, and people contemplating their existence late at night. We might find some answers in linear regression, of all the places.
Background
Linear regression can be simplified to drawing a line of best fit. Given any scattered set of data points, we want to create a line that best represents the shape of the data.
A simple, 2D linear regression model is given by a general form">
<meta name="author" content="Rohan Arni">
<link rel="canonical" href="http://localhost:1313/posts/free-will-regression/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.1cf6fbdbc3148f9ba6f6431621c28cfe4a5c7dff305cc1132deab8fbbd12ea54.css" integrity="sha256-HPb728MUj5um9kMWIcKM/kpcff8wXMETLeq4&#43;70S6lQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/free-will-regression/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="" id="
    top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
    <nav class="nav">
        <div class="logo">
            <a href="/">rohanarni.com</a>
                    <div class="logo-switches">
                        <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                            <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                                fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                                stroke-linejoin="round">
                                <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                            </svg>
                            <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                                fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                                stroke-linejoin="round">
                                <circle cx="12" cy="12" r="5"></circle>
                                <line x1="12" y1="1" x2="12" y2="3"></line>
                                <line x1="12" y1="21" x2="12" y2="23"></line>
                                <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                                <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                                <line x1="1" y1="12" x2="3" y2="12"></line>
                                <line x1="21" y1="12" x2="23" y2="12"></line>
                                <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                                <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                            </svg>
                        </button>
                        <ul class="lang-switch"><li>|</li>
                        </ul>
                    </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/newsletter/" title="Newsletter">
                    <span>Newsletter</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header><main class="main">

<article class="post-single">
    <header class="top" class="post-header">
        
        <h1 class="post-title">
            Linear Regression and Free Will
        </h1>
        <div class="post-meta"><span title='2025-07-24 09:10:42 -0500 -0500'>July 24, 2025</span>&nbsp;·&nbsp;Rohan Arni

</div>
    </header> 
    <div class="post-content"><p>Do we have free will? That’s a question that’s haunted many philosophers, scientists, and people contemplating their existence late at night. We might find some answers in linear regression, of all the places.</p>
<h3 id="background">Background<a hidden class="anchor" aria-hidden="true" href="#background">#</a></h3>
<p>Linear regression can be simplified to drawing a line of best fit. Given any scattered set of data points, we want to create a line that best represents the shape of the data.
A simple, 2D linear regression model is given by a general form</p>
<p>$$y = \beta_0 + \beta_1 x$$</p>
<p>Where $\beta_0, \beta_1$ are learnable parameters of the system. We can collect this into a matrix system of equations for $N$ finite data points:</p>
<p>$$\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\varepsilon}$$</p>
<p>Where:</p>
<ol>
<li>$\mathbf{y}$ is the $N\times 1$ matrix of observed dependent variables</li>
<li>$\mathbf{X}$ is the $N \times 2$ matrix of independent variables (the first column is all ones, the second column is the $x$ values)</li>
<li>$\mathbf{\beta}$ is the $2 \times 1$ matrix of coefficients</li>
<li>$\mathbf{\varepsilon}$ is the $N \times 1$ matrix of errors for each prediction
These matrices look something like:</li>
</ol>
<p>How do we go about learning the parameters $\mathbf{\beta}$ of the data? There are two approaches: solving the normal equations for least squares or performing gradient descent.</p>
<h3 id="least-squares">Least Squares<a hidden class="anchor" aria-hidden="true" href="#least-squares">#</a></h3>
<p>We want to minimize the sum of squared residuals (how inaccurate the model is compared to the true dependent variable:</p>
<p>$$S(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})$$</p>
<p>To minimize the loss function, we can take the derivative with respect to $\boldsymbol{\beta}$ to get</p>
<p>$$\frac{\partial S}{\partial \boldsymbol{\beta}} = -2 \mathbf{X}^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) = 0$$</p>
<p>Now, we can solve for $\boldsymbol{\beta}$ as</p>
<p>$$\mathbf{X}^\top \mathbf{y} - \mathbf{X}^{\top} \mathbf{X} \boldsymbol{\beta} = 0 $$$$
\mathbf{X}^\top \mathbf{y} = \mathbf{X}^{\top} \mathbf{X} \boldsymbol{\beta} $$$$
\boldsymbol{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$</p>
<p>This is an exact solution for our model parameter vector $\boldsymbol{\beta}$.</p>
<h3 id="gradient-descent">Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#gradient-descent">#</a></h3>
<p>We want to minimize this loss function called the mean squared error (MSE).</p>
<p>$$J(\boldsymbol{\beta}) = \frac{1}{2n} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})$$</p>
<p>Once again, we take the derivative of the loss function with respect to $\boldsymbol{\beta}$.</p>
<p>$$\nabla_{\boldsymbol{\beta}} J = -\frac{1}{n} \mathbf{X}^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})$$</p>
<ul>
<li>This gives the direction of steepest ascent, so for gradient descent we subtract it. Given some small learning rate $\alpha$, we update our parameters as follows
$$\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \alpha \nabla_{\boldsymbol{\beta}} J$$</li>
</ul>
<p>Or,</p>
<p>$$\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + \frac{\alpha}{n} \mathbf{X}^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}^{(t)})$$</p>
<p>We can repeat this process until the model converges to a minimum loss.</p>
<h3 id="free-will-and-regression">Free Will and Regression<a hidden class="anchor" aria-hidden="true" href="#free-will-and-regression">#</a></h3>
<p>There&rsquo;s a pretty interesting correlation between these methods of solving linear regression and some ideas relating to the problem of free will. There&rsquo;s a direct correlation between hard determinism (the idea that the universe is pre-determined and you have no individual free will) and the least squares solution to regression. Notice how the least squares solution is an exact, precise computation, giving an exact answer every time. This is  similar to the idea that we follow pre-determined paths to an end result, an idea that comes from hard determinism.</p>
<p>The gradient descent approach has more degrees of freedom (learning rate, number of iterations), and the model &ldquo;learns&rdquo; the parameters of the solution as guided by some &ldquo;cost&rdquo; function (in this case the MSE loss function). This idea is similar to the idea of soft determinism, where we have our own freedom to choose the actions but will lead to the same result as if the universe was deterministic. We have the freedom to choose our actions (each step of gradient descent represents an action we take), but will approach the same result as if the universe had a pre-determined result (least squares regression). However, the main difference is that gradient descent is not guaranteed to take the same path every time, and it can get stuck in local minima. This is similar to the idea that we have the freedom to choose our actions, but we may not always make the best choices.</p>
<p>In addition, following either approach implies that we are guided by some cost/loss function that we are perpetually optimizing. What are the implications of that notion? For another day&hellip;</p>
<hr>


        <br>
    </div>

    <center> <script async src="https://eocampaign1.com/form/1559edaa-677c-11f0-bbd4-eb13bbfcc584.js" data-form="1559edaa-677c-11f0-bbd4-eb13bbfcc584"></script></center>


    <footer class="post-footer" class="bottom">
        <ul class="post-tags">
        </ul>
    </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">rohanarni.com</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script></body>

</html>